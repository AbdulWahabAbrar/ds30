{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import onlineldavb\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "\n",
    "import simplejson\n",
    "import sys\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "from collections import Counter\n",
    "import heapq\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from itertools import islice, chain\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from bokeh import charts, plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def batch(iterable, size):\n",
    "    sourceiter = iter(iterable)\n",
    "    while True:\n",
    "        batchiter = islice(sourceiter, size)\n",
    "        yield chain([batchiter.next()], batchiter)\n",
    "        \n",
    "def nlargest(n, word_scores):\n",
    "    return heapq.nlargest(n, word_scores, key=lambda x: x[1])\n",
    "\n",
    "plotting.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"twitter_secrets.json.nogit\") as fh:\n",
    "    secrets = simplejson.loads(fh.read())\n",
    "\n",
    "auth = OAuth1(\n",
    "    secrets[\"api_key\"],\n",
    "    secrets[\"api_secret\"],\n",
    "    secrets[\"access_token\"],\n",
    "    secrets[\"access_token_secret\"]\n",
    ")\n",
    "\n",
    "def tweet_generator():\n",
    "    stream = requests.post('https://stream.twitter.com/1.1/statuses/filter.json',\n",
    "                         auth=auth,\n",
    "                         stream=True,\n",
    "                         data={\"locations\" : \"-125.00,24.94,-66.93,49.59\"})\n",
    "    \n",
    "    for line in stream.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        if not line:\n",
    "            continue\n",
    "        tweet = simplejson.loads(line)\n",
    "        if 'text' in tweet:\n",
    "            yield tweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"dictnostops.txt\") as fh:\n",
    "    words = [line.strip() for line in fh.readlines()]\n",
    "    word_to_index = { word: k for k, word in enumerate(words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DISPLAY_EVERY = 20\n",
    "\n",
    "tweets = 0\n",
    "counter = Counter()\n",
    "for tweet in tweet_generator():\n",
    "    for word in tweet.lower().split():\n",
    "        if word not in stop:\n",
    "            counter[word] += 1\n",
    "    tweets += 1\n",
    "    if tweets % DISPLAY_EVERY == (DISPLAY_EVERY - 1):\n",
    "        sys.stdout.write(\"\\r\" + str(nlargest(10, counter.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "CLUSTER_SIZE = 2\n",
    "\n",
    "cluster = MiniBatchKMeans(\n",
    "    n_clusters=CLUSTER_SIZE,\n",
    ")\n",
    "\n",
    "for tweets in batch(tweet_generator(), BATCH_SIZE):\n",
    "    mat = sp.sparse.dok_matrix((BATCH_SIZE, len(words)))\n",
    "    for row, tweet in enumerate(tweets):\n",
    "        for word in tweet.lower().split():\n",
    "            if word in word_to_index:\n",
    "                mat[row, word_to_index[word]] = 1.\n",
    "    cluster.partial_fit(mat.tocsr())\n",
    "    result = [\n",
    "        nlargest(5, zip(words, cluster.cluster_centers_[i]))\n",
    "        for i in xrange(cluster.n_clusters)\n",
    "    ]\n",
    "    sys.stdout.write(\"\\r\" + str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 2\n",
    "D = 1e9\n",
    "BATCH_SIZE = 20\n",
    "olda = onlineldavb.OnlineLDA(words, K, D, 1./K, 1./K, 1024., 0.7)\n",
    "\n",
    "for tweets in batch(tweet_generator(), BATCH_SIZE):\n",
    "    olda.update_lambda(list(tweets))\n",
    "    sys.stdout.write(\"\\r\" + str(olda.topic_words(5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
