{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science in 30 minutes: Neural networks and `word2vec`\n",
    "\n",
    "A _neural network_ is a method of machine learning which chains together a number of more basic models at a number of _layers_, with each layer feeding into the next. To make this more clear, let's look at the most basic (1-node, 1-layer) neural network, the \"perceptron\".\n",
    "\n",
    "## Neural Net Basics: The perceptron\n",
    "The _perceptron_ is a linear decision boundary classifier\\* that trains by an iterative learning approach.\n",
    "\n",
    "The model works as follows:\n",
    "\n",
    "- **Input**: A data point. This point is transformed into an $n$-length \"feature vector\" $v$ $\\in R^n$, with each element describing the value of that particular feature.\n",
    "- **Output**: A classification, either -1 or 1.\n",
    "\n",
    "As mentioned above, the basic perceptron is linear, which means we can represent our model as another $n$-length \"weight vector\" $w$ - in the image below with input vector $v$, we\n",
    " - compute the inner product $<v,w> := u$\n",
    " - calculate $f(u)$, where $f$ is the _activation function_ (in a perceptron, the Heaviside step function, as in the diagram below).\n",
    " \n",
    " to obtain our prediction. It may also be instructive to think of this as matrix multiplication (with $v$ a `1xn` matrix and `w^T` an `nx1` matrix)- when we chain together perceptrons for a neural net, we can in fact represent each layer of the model as a matrix.\n",
    "\n",
    "![Perceptron](http://i.stack.imgur.com/KUvpQ.png)\n",
    "\n",
    "\\*In fact, one can use [kernel methods](https://en.wikipedia.org/wiki/Kernel_perceptron) (much like with SVMs) to attempt nonlinear classification with perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: \n",
    "\n",
    "#### In theory\n",
    "Mathematically, whenever we perform supervised learning, we seek to minimize an _objective function_ which essentially measures how closely our model describes the training data.\\*\n",
    "\n",
    "While the standard perceptron's training algorithm provably converges on linearly separable data, our function is not smooth (due to the use of the Heaviside function), and the proof is not immediate from standard convex optimization methods.\n",
    "\n",
    "Let's look at a slightly modified perceptron using the [logistic sigmoid function](http://mathworld.wolfram.com/SigmoidFunction.html) $\\sigma$ as the activation function instead of the Heaviside function, and derive our training method from there.\n",
    "\n",
    "Here's our objective function: \n",
    "\n",
    "$$ E(\\vec{\\mathbf{w}}) = \\sum_{i=1}^{|T|} \\frac{1}{2} (t_i - y_i) ^2 $$\n",
    "\n",
    "Where $t_i$ is our gold standard datum, and $y_i = \\sigma (\\vec{\\mathbf{w}} \\cdot \\vec{x}_i)$. For simpler derivation later, let $u(\\vec{x}_i) = \\vec{\\mathbf{w}} \\cdot \\vec{x}_i$.\n",
    "\n",
    "As it turns out, this is the objective function for a very familiar model - [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). We can use the usual convex optimization methods - namely [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) - to train our model.\n",
    "\n",
    "To see this, let's take $\\frac{\\partial{E}}{\\partial{w_j}}$. Noting that differentiation is linear and using the chain rule, it's easy to see that\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{w_j}} = \n",
    "        \\sum_{i} \\frac{\\partial{E}}{\\partial{y_i}} \\cdot \\frac{\\partial{y_i}}{\\partial{u_i}} \\cdot \\frac{\\partial{u_i}}{\\partial{w_j}} $$\n",
    "Here's each of those terms, from the equation above (and noting $\\frac{d\\sigma}{dx} = \\sigma(x) ( 1 - \\sigma(x))$:\n",
    "\n",
    "$$ \\frac{\\partial{E}}{\\partial{y_i}} = (t_i - y_i) \\\\\n",
    "   \\frac{\\partial{y_i}}{\\partial{u_i}} =  (y_i)(1-y_i) \\\\\n",
    "   \\frac{\\partial{u_i}}{\\partial{w_j}} = x_i\n",
    "$$\n",
    "Because we're using SGD, we don't need to take the gradient with respect to the entire function - just the single example $x_i$ we plan to iteratively update $\\vec{\\mathbf{w}}$ with.\n",
    "\n",
    "Hence at each update phase, we update our weights in the direction of the local gradient as follows:\n",
    "\n",
    "$$ \\vec{\\mathbf{w_0}} = [0 ..... 0]\\\\\n",
    "\\vec{\\mathbf{w_{i+1}}} = \\vec{\\mathbf{w_i}} + \\eta  \\cdot (t_i - y_i)(y_i)(1 - y_i) \\vec{x}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the _learning rate_, a parameter we choose (between 0 and 1) that determines the rate at which our weight vector updates after seeing each training example.\n",
    "\n",
    "#### In practice\n",
    "Let's show a low-dimensional example of the algorithm we just derived. We train the sigmoid perceptron by iterating over all the data multiple times, and updating our weight vector $w$ as we see each data point. We call 1 full iteration over the training set an _epoch_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy.special import expit as sig\n",
    "\n",
    "random.seed(42)\n",
    "training_x  = np.array(zip(2*random.randn(100), 2*random.randn(100)))\n",
    "training_y = [0 if y[1]  > y[0] else 1 for y in training_x]\n",
    "training_data = zip(training_x, training_y)\n",
    "plt.scatter([x[0] for x in training_x], [y[1] for y in training_x],  c=training_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above chart, we've created some random data points in a 2-dimensional vector space and classified them relative to the line $y = x$ (points \"above\" are class 0, points \"below\" are class 1). Now, we can train our perceptron on this clearly linearly separable data, and check how well it did..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 3000\n",
    "weights = np.array([.15, .15])\n",
    "learning_rate = .001\n",
    "for epoch in xrange(epochs):\n",
    "    for x_vector, t in training_data:\n",
    "        pred = sig(np.inner(x_vector, weights))\n",
    "        update = learning_rate * pred * ( 1 - pred) * (pred - t)\n",
    "        weights = weights - (update * x_vector)\n",
    "    if epoch > 0 and epoch % 1000 == 0:\n",
    "        print \"weights after epoch %s : %s\" % (str(epoch), str(weights))\n",
    "        \n",
    "        \n",
    "actuals = [t[1] for t in training_data]\n",
    "preds = [1 if sigmoid(np.inner(t[0], weights)) > .5 else 0 for t in training_data]\n",
    "\n",
    "print \"Mean Absolute Error:\", metrics.mean_absolute_error(actuals, preds)\n",
    "print \"Mean Squared Error:\", metrics.mean_squared_error(actuals, preds)\n",
    "print \"R^2:\", metrics.r2_score(actuals, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly what we'd expect - a perfect fit! Now let's plot the separating hyperplane the model developed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We rotate the resultant vector 90 degrees. \n",
    "m = -(weights[0])/(weights[1])\n",
    "#Since we didn't include a bias term, we start at (0,0)\n",
    "p1 = [0, 0]\n",
    "x2 = 1.3\n",
    "y2 = (x2 - p1[0])*m + p1[1]\n",
    "p2 = [x2, y2]\n",
    "\n",
    "def drawLine2P(x,y,xlims):\n",
    "    xrange = np.arange(xlims[0],xlims[1],0.1)\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    k, b = np.linalg.lstsq(A, y)[0]\n",
    "    plt.plot(xrange, k*xrange + b, 'k')\n",
    "\n",
    "plt.scatter([x[0] for x in training_x], [y[1] for y in training_x],  c=training_y)\n",
    "drawLine2P([p1[0], p2[0]], [p1[1], p2[1]], [-6, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pitfalls:\n",
    "The perceptron is too simple of a model for more complex classification tasks. However, chaining perceptrons into a neural network allows for surprisingly good results across a wide variety of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Perceptrons into Neural Networks\n",
    "\n",
    "In the below diagram (a visual representation of a neural net), you can think of each \"hidden node\" as a perceptron that takes the average of the input layers' values as input and produces a unique prediction of its own:\n",
    "![A Neural Network](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "That said, there are a couple of important differences, mathematically speaking:\n",
    "\n",
    "1. As mentioned above, perceptrons typically use the Heaviside step function for activation. In neural nets, the logistic sigmoid function is typical. \n",
    "2. We derived the single node sigmoid training algorithm above - the SGD-based method we use to train a multi-layer neural network is commonly known _backpropogation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "As it turns out, the skip-gram and continuous-bag-of-words neural networks used in [word2vec](https://code.google.com/p/word2vec/) are simple neural networks that produce a suprisingly cool result on natural language data. \n",
    "\n",
    "word2vec is a library that, given a corpus of natural language text data (think Wikipedia articles), maps each word into a high-dimensional vector space. For those of you familiar with information retrieval theory, the concept is widely used in that field: a word's position in the vector space denotes how _similar_ it is to other nearby words. \n",
    "\n",
    "In this way we can create interesting clusters of related words, and, somewhat more compellingly, learn nontrivial associations between words.\n",
    "\n",
    "For example, consider the analogy, \n",
    "\n",
    "\"Man is to King as Woman is to \\_\\_\\_\\_\\_\\_\\_\"\n",
    "\n",
    "As an English speaker, you know the answer is \"Queen\". As it turns out, a well-trained word2vec vector space can also come to this conclusion by the following vector computation:\n",
    "\n",
    "` closest( (vector(\"king\") - vector(\"man\")) + vector(\"woman\") )`\n",
    "\n",
    "Where we measure closeness via cosine similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The layers\n",
    "As is typical of simple neural nets, CBOW has an _input_ layer, a _hidden_ layer, and an _output_ layer. A linear transformation (ie a matrix) governs the movement from input to hidden, and another handles movement from hidden to output - the above diagram labels these transformations $\\mathbf{W}$ and $\\mathbf{W'}$. In CBOW, we have\n",
    "\n",
    "- **input**: Four _one-hot encoded_\\* words - i.e. $1 x V$ vectors where $V$ is the size of our vocabulary, all values are 0 except for one.\n",
    "- $\\mathbf{W}$: A $V x N$ matrix that transitions into the hidden layer, and notably maps our encoded word into an $N$-dimensional (usually $N\\approx300$) vector space. In otherwords, after we've completed training this model, we'll /only/ care about this matrix and the embedding it creates for our vocabulary.\n",
    "- **hidden**: A $1 x N$ vector that represents the mapping of our input word into the desired vector space\n",
    "- $\\mathbf{W'}$: An $N x V$ matrix transitioning between the hidden layer and the output.\n",
    "- **output**: A single one-hot encoded word - the model's prediction for most likely to occur given the surrounding four-word context.\n",
    "\n",
    "\\* This isn't strictly true as word2vec actually uses a different, optimized tree structure - but the one-hot encoding is illustrative.\n",
    "\n",
    "Note that $\\mathbf{W}$ is the artifact we're looking for, as it transitions our encoded words into an interesting vector space. Hence, to develop $\\mathbf{W}$, we must simply train this model and recover the weight parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try it!\n",
    "We decided to try this on some real-world data, in particular Yelp's [Academic Dataset](https://www.yelp.com/academic_dataset), to see if we could come up with anything interesting.\n",
    "\n",
    "Some questions we can ask via this analogy structure:\n",
    "\n",
    "- What's the closest thing to Pizza in France? \n",
    "- What is non-spicy Mexican food?\n",
    "- The tagine is emblematic of North African cuisine: is there anything similar in other geographies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tool: `gensim`\n",
    "Gensim is a great python library for various topic-modelling and clustering tasks. The library recently added word2vec to its toolbox - it's a thin wrapper around the highly optimized [C implementation](https://code.google.com/p/word2vec/). \n",
    "\n",
    "To install gensim, you can simply run `pip install gensim`. You'll need:\n",
    "\n",
    "- at least `numpy/scipy` installed\n",
    "- ideally a C compiler so that you can use optimized word2vec training (as noted [here](https://radimrehurek.com/gensim/models/word2vec.html)).\n",
    "\n",
    "## The dataset: The Yelp Academic Datset \n",
    "\n",
    "We host a version of [Yelp's Academic Dataset](https://www.yelp.com/academic_dataset) for our fellows. \n",
    "\n",
    "We'll also use the first [Pre-trained model](https://code.google.com/p/word2vec/#Pre-trained_word_and_phrase_vectors) (from the Google News Dataset), from the `word2vec` Google Code page above, to illustrate some of the classic analogies we mentioned above. Note these vectors are 1.5GB, so sit back and relax! \n",
    "\n",
    "To follow along, you should\n",
    "\n",
    "- Download the Yelp review data for yourself.\n",
    "- If you want to see the classic analogies and play with that data a bit more, download the Google News vectors as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The code - pre-trained model.\n",
    "\n",
    "First, let's look at the pre-trained model and see what fun analogies we can come up with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "googlenews_model = gensim.models.word2vec.Word2Vec.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\",\n",
    "                                                                        binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googlenews_model.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If asked to write down some words associated with royalty, you or I would probably write down similar lists. \n",
    "\n",
    "Clustering, however, is a long-solved problem - [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), for example, could produce similar lists for us if trained on this dataset. What's particularly interesting about `word2vec` is the following sort of computation:\n",
    "\n",
    "$$ vec(king) - vec(man) + vec(woman) $$\n",
    "\n",
    "When we perform this vector operation, what words are closest to the resultant vector? Are they meaningful? Let's try! \n",
    "\n",
    "We'll start with the example we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec(king) - vec(man) + vec(woman)\n",
    "googlenews_model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that \"monarch\", a gender-neutral term, is also listed as quite similar. Does this hold when we subtract `vec(man)` without adding in `vec(woman)`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "googlenews_model.most_similar(positive=[\"king\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite - though \"kingdom\" is another interesting similarity when we think of \"man\" as referring to a human.\n",
    "\n",
    "Let's look at what clusters around something we'll try on the Yelp Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec('burrito') - vec('mexican') + vec('chinese')\n",
    "googlenews_model.most_similar(positive=[\"burrito\", \"chinese\"], negative=[\"mexican\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, interestingly, what the \"default\" burrito is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec('burrito') - vec('mexican')\n",
    "googlenews_model.most_similar(positive=[\"burrito\"], negative=[\"mexican\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code: Training the model.\n",
    "Now that we've looked at a well-trained model, let's try some modelling of our own!\n",
    "\n",
    "Our text processing leaves something to be desired. However, one of the coolest things about `word2vec` is that even with the most trivial of input cleanings, we get interesting results, as you'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: better text processing\n",
    "import simplejson as json\n",
    "reviews = []\n",
    "lines = open(\"yelp_train_academic_dataset_review.json\")\n",
    "for line in lines:\n",
    "    reviews.append(json.loads(line))\n",
    "reviews = [r['text'].lower().split(\" \") for r in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this took about an hour on our 16GB Digital Ocean instance - go grab lunch!\n",
    "yelp_model = gensim.models.word2vec.Word2Vec(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can configure logs to send output to a file if you like - the [gensim docs](https://radimrehurek.com/gensim/tutorial.html) explain how to activate logging if that's something that interests you. Here's what training output looks like when using Spark's word2vec implementation:\n",
    "\n",
    "![Training output](progress.png)\n",
    "\n",
    "wordCount is how many words we've seen, and alpha is the learning rate - a useful optimization for training neural nets is to incrementally decrease the learning rate as time goes on and we converge toward the objective function's minimum. On our local setup with gensim (using a [16gb DigitalOcean instance](https://www.digitalocean.com/pricing/)), training on this dataset takes about 30 minutes.\n",
    "\n",
    "Once the model is trained, we can write it out to a file to load up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_model.save(\"yelp_word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a Trained Model\n",
    "\n",
    "Once we have the model trained, we can begin to query the vector space for interesting results and similarities. `model.most_similar` lets us pass in a word to get the closest vectors to that word (if it exists in the dataset). Take a look below at some of the immediate results we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "yelp_model = gensim.models.word2vec.Word2Vec.load(\"yelp_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_model.most_similar(\"chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_model.most_similar(\"mexican\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the above is more or less what we'd expect from LDA on this dataset.\n",
    "\n",
    "Is there anything in Chinese cuisine similar to a burrito?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_model.most_similar(positive=[\"chinese\", \"burrito\"], negative=[\"mexican\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much here - vermicelli are noodles, beef/chicken might be getting to something - but this is an early hint that our dataset might not be big enough to effectively build an interesting vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_model.most_similar(\"pizza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we have some tokenizing issues, but more or less what we'd expect. Maybe we can try looking for pizza styles/places to avoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp_model.most_similar(positive=[\"pizza\", \"nyc\", \"bad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: Avoid Grimaldi's, and maybe difara's! Maybe pizza in Brooklyn? (This is probably an artifact of the \"nyc\" vector's weight).\n",
    "\n",
    "## The dataset itself and future exploration\n",
    "\n",
    "This dataset itself has an issue we can't resolve with better parsing or parameter tweaking: it's too small to train a truly effective word2vec model. We got some fun and interesting results, but really, we'd want to run this on something closer to all of Yelp's restaurant review text to develop some really cool analogies. Of course, Yelp doesn't simply make all of its data available for free - however, Yelp is one of our hiring partners ;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References and useful links\n",
    "  \n",
    "http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf - a good explanation of word2vec including a succinct overview of backpropogation, as well as `wevi`, the tool we used to visualize the algorithm and its embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
